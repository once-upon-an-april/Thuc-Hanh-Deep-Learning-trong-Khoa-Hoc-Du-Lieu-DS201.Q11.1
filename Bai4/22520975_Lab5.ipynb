{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMfFx7IMpb0ihyx4BgPA4GL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/once-upon-an-april/Thuc-Hanh-Deep-Learning-trong-Khoa-Hoc-Du-Lieu-DS201.Q11.1/blob/main/Bai4/22520975_Lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bài 1: Xây dựng mô hình Transformer Encoder gồm 3 lớp theo mô tả trong nghiên cứu [Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). Huấn luyện mô hình này cho bài toán phân loại domain câu bình luận trên bộ dữ liệu [UIT-ViOCD](https://drive.google.com/drive/folders/1Lu9axyLkw7dMx80uLRgvCnZsmNzhJWAa?usp=sharing)."
      ],
      "metadata": {
        "id": "-aoIlWc-jGPV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT7jSulVfZhO"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "try:\n",
        "  ds = load_dataset(\"tarudesu/ViOCD\", trust_remote_code=True)\n",
        "\n",
        "  print(\"Load dataset thành công!\")\n",
        "  print(ds)\n",
        "\n",
        "  if 'train' in ds:\n",
        "    print(f\"Số lượng mẫu train: {len(ds['train'])}\")\n",
        "    print(\"Ví dụ mẫu đầu tiên:\", ds['train'][0])\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"Có lỗi xảy ra: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df_train = ds['train'].to_pandas()\n",
        "\n",
        "# Hiển thị 5 mẫu ngẫu nhiên\n",
        "print(df_train.sample(5)[['review', 'review_tokenize', 'domain', 'label']])"
      ],
      "metadata": {
        "id": "mnMStb71lRSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "domain_counts = df_train['domain'].value_counts()\n",
        "\n",
        "print(\"Phân bố dữ liệu:\", domain_counts)\n",
        "\n",
        "# Vẽ biểu đồ\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=domain_counts.index, y=domain_counts.values, palette='viridis')\n",
        "plt.title('Số lượng mẫu theo từng domain')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H7Tp4FCqmAZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['length'] = df_train['review_tokenize'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "print(df_train['length'].describe())\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df_train['length'], bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title('Phân phối độ dài câu')\n",
        "plt.xlabel('Số từ')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5QPi-e6fmvuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Số lượng giá trị null:\\n\", df_train.isnull().sum())\n",
        "\n",
        "empty_reviews = df_train[df_train['review_tokenize'].str.strip() == '']\n",
        "print(f\"Số lượng câu rỗng: {len(empty_reviews)}\")"
      ],
      "metadata": {
        "id": "e2lyIOqSnr-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len=500):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    # Tạo ma trận vị trí\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_head):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    assert d_model % n_head == 0\n",
        "\n",
        "    self.d_k = d_model // n_head\n",
        "    self.n_head = n_head\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "    self.fc_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    batch_size = q.size(0)\n",
        "\n",
        "    Q = self.w_q(q).view(batch_size, -1, self.n_head, self.d_k).transpose(1, 2)\n",
        "    K = self.w_k(k).view(batch_size, -1, self.n_head, self.d_k).transpose(1, 2)\n",
        "    V = self.w_v(v).view(batch_size, -1, self.n_head, self.d_k).transpose(1, 2)\n",
        "\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    attn_probs = torch.softmax(scores, dim=-1)\n",
        "\n",
        "    output = torch.matmul(attn_probs, V)\n",
        "\n",
        "    output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "    return self.fc_out(output)\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "        return self.fc2(self.dropout(F.relu(self.fc1(x))))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_head, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, n_head)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.mha(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_head, n_layers, n_classes, max_len=256, dropout=0.1):\n",
        "        super(TransformerEncoderClassifier, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, n_head, d_ff=d_model*4, dropout=dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, n_classes)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model) # Scaling embedding theo paper\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        return self.fc_out(x)"
      ],
      "metadata": {
        "id": "o2PQ12UXq-Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.optim as optim\n",
        "\n",
        "# Xây dựng Vocabulary từ tập train\n",
        "def build_vocab(dataset):\n",
        "    all_tokens = []\n",
        "    for text in dataset['review_tokenize']:\n",
        "        all_tokens.extend(text.lower().split())\n",
        "\n",
        "    token_counts = Counter(all_tokens)\n",
        "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "    for token, count in token_counts.items():\n",
        "        if count >= 2:\n",
        "            vocab[token] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(ds['train'])\n",
        "print(f\"Kích thước từ điển: {len(vocab)}\")\n",
        "\n",
        "# Label Map (Domain -> Index)\n",
        "domains = ['app', 'fashion', 'cosmetic', 'mobile']\n",
        "label_map = {domain: i for i, domain in enumerate(domains)}\n",
        "print(f\"Label Map: {label_map}\")\n",
        "\n",
        "# Hàm xử lý Batch\n",
        "MAX_LEN = 256\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for _item in batch:\n",
        "        label_list.append(label_map[_item['domain']])\n",
        "\n",
        "        tokens = [vocab.get(token, vocab['<UNK>']) for token in _item['review_tokenize'].lower().split()]\n",
        "\n",
        "        if len(tokens) > MAX_LEN:\n",
        "            tokens = tokens[:MAX_LEN]\n",
        "\n",
        "        text_list.append(torch.tensor(tokens, dtype=torch.long))\n",
        "\n",
        "    text_list = pad_sequence(text_list, batch_first=True, padding_value=vocab['<PAD>'])\n",
        "    label_list = torch.tensor(label_list, dtype=torch.long)\n",
        "\n",
        "    return text_list, label_list\n",
        "\n",
        "# Tạo DataLoader\n",
        "BATCH_SIZE = 64 # Tăng lên 64 vì câu ngắn (mean=27)\n",
        "train_loader = DataLoader(ds['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "val_loader = DataLoader(ds['validation'], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(ds['test'], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Khởi tạo mô hình\n",
        "model = TransformerEncoderClassifier(\n",
        "    vocab_size=len(vocab),\n",
        "    d_model=128,\n",
        "    n_head=4,\n",
        "    n_layers=3,\n",
        "    n_classes=len(domains),\n",
        "    max_len=MAX_LEN,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "# Tính toán Class Weights để xử lý mất cân bằng dữ liệu\n",
        "class_weights = torch.tensor([0.68, 0.80, 1.05, 3.02], dtype=torch.float).to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4) # Learning rate nhỏ cho Transformer\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for text, labels in loader:\n",
        "        text, labels = text.to(device), labels.to(device)\n",
        "\n",
        "        # Tạo mask cho phần padding (PAD = 0)\n",
        "        # Mask shape: (Batch, 1, 1, Seq_Len)\n",
        "        mask = (text != vocab['<PAD>']).unsqueeze(1).unsqueeze(2).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(text, mask)\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Hàm đánh giá\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    preds, targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text, labels in loader:\n",
        "            text, labels = text.to(device), labels.to(device)\n",
        "            mask = (text != vocab['<PAD>']).unsqueeze(1).unsqueeze(2).to(device)\n",
        "\n",
        "            output = model(text, mask)\n",
        "            loss = criterion(output, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            pred = torch.argmax(output, dim=1)\n",
        "            preds.extend(pred.cpu().numpy())\n",
        "            targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    return total_loss / len(loader), preds, targets\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "print(f\"Bắt đầu huấn luyện trên {device}...\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss, _, _ = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "test_loss, preds, targets = evaluate(model, test_loader, criterion)\n",
        "print(\"\\n=== KẾT QUẢ TRÊN TẬP TEST ===\")\n",
        "print(classification_report(targets, preds, target_names=domains))"
      ],
      "metadata": {
        "id": "za-b-Hb1KJeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bài 2: Xây dựng mô hình Transformer Encoder gồm 3 lớp theo mô tả trong nghiên cứu [Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). Huấn luyện mô hình này cho bài toán gán nhãn chuỗi trên bộ dữ liệu [PhoNERT](https://github.com/VinAIResearch/PhoNER_COVID19)."
      ],
      "metadata": {
        "id": "FudYXzl4LPsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def load_phoner_data(url):\n",
        "    response = requests.get(url)\n",
        "    data = []\n",
        "    for line in response.text.strip().split('\\n'):\n",
        "        try:\n",
        "            data.append(json.loads(line))\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "    df = pd.DataFrame.from_dict(data)\n",
        "    return df\n",
        "\n",
        "print(\"Đang tải dữ liệu PhoNERT...\")\n",
        "base_url = \"https://raw.githubusercontent.com/VinAIResearch/PhoNER_COVID19/main/data/syllable/\"\n",
        "\n",
        "train_df = load_phoner_data(base_url + \"train_syllable.json\")\n",
        "val_df = load_phoner_data(base_url + \"dev_syllable.json\")\n",
        "test_df = load_phoner_data(base_url + \"test_syllable.json\")\n",
        "\n",
        "print(f\"Số lượng mẫu Train: {len(train_df)}\")\n",
        "print(f\"Số lượng mẫu Val: {len(val_df)}\")\n",
        "print(f\"Số lượng mẫu Test: {len(test_df)}\")\n",
        "\n",
        "print(\"\\n--- Mẫu dữ liệu đầu tiên ---\")\n",
        "print(\"Câu:\", train_df.iloc[0]['words'])\n",
        "print(\"Nhãn:\", train_df.iloc[0]['tags'])"
      ],
      "metadata": {
        "id": "0tHoxjA2K7sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Xây dựng Vocabulary\n",
        "def build_vocab(text_lists):\n",
        "    all_tokens = [token.lower() for seq in text_lists for token in seq]\n",
        "    token_counts = Counter(all_tokens)\n",
        "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "    for token, count in token_counts.items():\n",
        "        if count >= 2:\n",
        "            vocab[token] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(train_df['words'])\n",
        "print(f\"Kích thước từ điển: {len(vocab)}\")\n",
        "\n",
        "# 2. Xây dựng Tag Map\n",
        "# Lấy tất cả các nhãn xuất hiện trong tập train\n",
        "all_tags = sorted(list(set([tag for seq in train_df['tags'] for tag in seq])))\n",
        "tag_map = {tag: i for i, tag in enumerate(all_tags)}\n",
        "print(f\"Số lượng nhãn ({len(tag_map)}): {tag_map}\")\n",
        "\n",
        "# 3. Dataset Class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, df, vocab, tag_map):\n",
        "        self.df = df\n",
        "        self.vocab = vocab\n",
        "        self.tag_map = tag_map\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words = self.df.iloc[idx]['words']\n",
        "        tags = self.df.iloc[idx]['tags']\n",
        "\n",
        "        # Chuyển từ -> index\n",
        "        word_ids = [self.vocab.get(w.lower(), self.vocab['<UNK>']) for w in words]\n",
        "        # Chuyển tag -> index\n",
        "        tag_ids = [self.tag_map[t] for t in tags]\n",
        "\n",
        "        return torch.tensor(word_ids, dtype=torch.long), torch.tensor(tag_ids, dtype=torch.long)\n",
        "\n",
        "# 4. Collate Function (Xử lý Padding)\n",
        "def collate_fn(batch):\n",
        "    text_list, label_list = [], []\n",
        "    for _text, _label in batch:\n",
        "        text_list.append(_text)\n",
        "        label_list.append(_label)\n",
        "\n",
        "    # Pad Text với 0 (<PAD>)\n",
        "    text_padded = pad_sequence(text_list, batch_first=True, padding_value=vocab['<PAD>'])\n",
        "\n",
        "    label_padded = pad_sequence(label_list, batch_first=True, padding_value=-100)\n",
        "\n",
        "    return text_padded, label_padded\n",
        "\n",
        "# Tạo DataLoader\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(NERDataset(train_df, vocab, tag_map), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(NERDataset(val_df, vocab, tag_map), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(NERDataset(test_df, vocab, tag_map), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(\"Đã tạo xong DataLoader!\")"
      ],
      "metadata": {
        "id": "6Z01PpxmMK28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class TransformerTokenClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_head, n_layers, n_classes, max_len=500, dropout=0.1):\n",
        "        super(TransformerTokenClassifier, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # Stack các Encoder Layer\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, n_head, d_ff=d_model*4, dropout=dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Output Layer: Chiếu vector d_model về số lượng nhãn (n_classes) cho từng token\n",
        "        self.fc_out = nn.Linear(d_model, n_classes)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # x: (Batch, Seq_Len)\n",
        "\n",
        "        # 1. Embedding + Positional\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # 2. Encoder Layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        # 3. Classifier\n",
        "        # x shape: (Batch, Seq_Len, d_model) -> (Batch, Seq_Len, n_classes)\n",
        "        output = self.fc_out(x)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "RdCXcKamMY9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "D_MODEL = 128\n",
        "N_HEAD = 4\n",
        "N_LAYERS = 3\n",
        "NUM_TAGS = len(tag_map)\n",
        "LR = 1e-4\n",
        "EPOCHS = 30\n",
        "\n",
        "# Khởi tạo mô hình\n",
        "model = TransformerTokenClassifier(len(vocab), D_MODEL, N_HEAD, N_LAYERS, NUM_TAGS).to(device)\n",
        "\n",
        "# Loss function bỏ qua padding (-100)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "print(f\"Mô hình đang chạy trên: {device}\")\n",
        "\n",
        "# Hàm train\n",
        "def train_ner(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for text, labels in loader:\n",
        "        text, labels = text.to(device), labels.to(device)\n",
        "\n",
        "        # Tạo mask (Batch, 1, 1, Seq_Len)\n",
        "        mask = (text != vocab['<PAD>']).unsqueeze(1).unsqueeze(2).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(text, mask)\n",
        "        # Output: (Batch, Seq, Tags), Label: (Batch, Seq)\n",
        "\n",
        "        # Flatten để tính Loss\n",
        "        loss = criterion(output.view(-1, NUM_TAGS), labels.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Hàm đánh giá\n",
        "def evaluate_ner(model, loader):\n",
        "    model.eval()\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text, labels in loader:\n",
        "            text, labels = text.to(device), labels.to(device)\n",
        "            mask = (text != vocab['<PAD>']).unsqueeze(1).unsqueeze(2).to(device)\n",
        "\n",
        "            output = model(text, mask)\n",
        "            preds = torch.argmax(output, dim=-1) # (Batch, Seq)\n",
        "\n",
        "            # Lọc bỏ padding (-100) để tính toán chính xác\n",
        "            for i in range(text.size(0)):\n",
        "                # Lấy chiều dài thực của câu (không tính pad)\n",
        "                valid_len = (labels[i] != -100).sum()\n",
        "\n",
        "                # Chỉ lấy phần nhãn thực\n",
        "                p = preds[i, :valid_len].cpu().numpy()\n",
        "                t = labels[i, :valid_len].cpu().numpy()\n",
        "\n",
        "                pred_labels.extend(p)\n",
        "                true_labels.extend(t)\n",
        "\n",
        "    return true_labels, pred_labels\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    loss = train_ner(model, train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {loss:.4f}\")\n",
        "\n",
        "# Đánh giá cuối cùng trên tập Test\n",
        "print(\"\\n--- KẾT QUẢ TRÊN TẬP TEST ---\")\n",
        "true_tags, pred_tags = evaluate_ner(model, test_loader)\n",
        "\n",
        "inv_tag_map = {v: k for k, v in tag_map.items()}\n",
        "true_tag_names = [inv_tag_map[i] for i in true_tags]\n",
        "pred_tag_names = [inv_tag_map[i] for i in pred_tags]\n",
        "\n",
        "print(classification_report(true_tag_names, pred_tag_names))"
      ],
      "metadata": {
        "id": "6BnJIf2rMl9A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}