{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyO1RxRVr/es+HbX/MKHaUos",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/once-upon-an-april/Thuc-Hanh-Deep-Learning-trong-Khoa-Hoc-Du-Lieu-DS201.Q11.1/blob/main/Bai2/22520975_Lab3_Bai2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wc8TyGJQ8MGN"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BÀI 2: PHÂN LOẠI CẢM XÚC TRÊN DATASET UIT-VSFC SỬ DỤNG GRU\n",
        "# (5 Lớp, Hidden Size 256, Adam Optimizer, F1 Score)\n",
        "# ==============================================================================\n",
        "!pip install pyvi\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from datasets import load_dataset\n",
        "from pyvi import ViTokenizer\n",
        "\n",
        "# Kiểm tra thiết bị (GPU/CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHẦN 1: TẢI VÀ TIỀN XỬ LÝ DỮ LIỆU (SỬ DỤNG HUGGING FACE DATASETS)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n>>> 1. Đang tải dataset từ HuggingFace...\")\n",
        "\n",
        "# Tải dataset từ HuggingFace (ưu tiên bản Parquet để tránh lỗi script bảo mật)\n",
        "try:\n",
        "    ds = load_dataset(\"uitnlp/vietnamese_students_feedback\", name=\"default\", revision=\"refs/convert/parquet\")\n",
        "except Exception:\n",
        "    # Fallback: Dùng trust_remote_code nếu không tải được bản parquet\n",
        "    ds = load_dataset(\"uitnlp/vietnamese_students_feedback\", trust_remote_code=True)\n",
        "\n",
        "# Chuyển đổi sang Pandas DataFrame để xử lý tiếp\n",
        "train_df = pd.DataFrame(ds['train'])\n",
        "val_df = pd.DataFrame(ds['validation'])\n",
        "test_df = pd.DataFrame(ds['test'])\n",
        "\n",
        "print(f\"Dataset Loaded. Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "# Hàm tiền xử lý\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
        "    text = ViTokenizer.tokenize(text) # Tách từ tiếng Việt\n",
        "    return text\n",
        "\n",
        "print(\">>> Preprocessing (Clean & Tokenize)...\")\n",
        "train_df['sentence_cleaned'] = train_df['sentence'].apply(preprocess_text)\n",
        "val_df['sentence_cleaned'] = val_df['sentence'].apply(preprocess_text)\n",
        "test_df['sentence_cleaned'] = test_df['sentence'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "cX7zVNhl9O0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHẦN 2: XÂY DỰNG TỪ ĐIỂN & DATASET\n",
        "# ==============================================================================\n",
        "\n",
        "# Cấu hình Hyperparameters\n",
        "MAX_LEN = 50\n",
        "BATCH_SIZE = 64\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_SIZE = 256  # Yêu cầu đề bài\n",
        "NUM_LAYERS = 5     # Yêu cầu đề bài\n",
        "NUM_CLASSES = 3\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 100\n",
        "\n",
        "print(\"\\n>>> 2. Xây dựng Vocabulary...\")\n",
        "all_words = \" \".join(train_df['sentence_cleaned']).split()\n",
        "word_counts = Counter(all_words)\n",
        "vocab = sorted([w for w, c in word_counts.items() if c >= 2])\n",
        "\n",
        "word2idx = {w: i+2 for i, w in enumerate(vocab)}\n",
        "word2idx['<PAD>'] = 0\n",
        "word2idx['<UNK>'] = 1\n",
        "\n",
        "print(f\"Vocab Size: {len(word2idx)}\")\n",
        "\n",
        "def vectorize(text, word2idx, max_len):\n",
        "    words = text.split()\n",
        "    idxs = [word2idx.get(w, word2idx['<UNK>']) for w in words]\n",
        "    if len(idxs) < max_len:\n",
        "        idxs = idxs + [word2idx['<PAD>']] * (max_len - len(idxs))\n",
        "    else:\n",
        "        idxs = idxs[:max_len]\n",
        "    return idxs\n",
        "\n",
        "class VSFC_Dataset(Dataset):\n",
        "    def __init__(self, df, word2idx, max_len):\n",
        "        self.sentences = df['sentence_cleaned'].values\n",
        "        self.labels = df['sentiment'].values\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vec = vectorize(self.sentences[idx], self.word2idx, self.max_len)\n",
        "        label = self.labels[idx]\n",
        "        return torch.tensor(vec, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(VSFC_Dataset(train_df, word2idx, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(VSFC_Dataset(val_df, word2idx, MAX_LEN), batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(VSFC_Dataset(test_df, word2idx, MAX_LEN), batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "O9SABUvn9P9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHẦN 3: XÂY DỰNG MODEL GRU\n",
        "# ==============================================================================\n",
        "\n",
        "class SentimentGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_dim, n_layers):\n",
        "        super(SentimentGRU, self).__init__()\n",
        "\n",
        "        # 1. Embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # 2. GRU Layer (Thay thế LSTM bằng GRU)\n",
        "        # GRU không có cell state, chỉ có hidden state\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.3\n",
        "        )\n",
        "\n",
        "        # 3. Output Layer\n",
        "        self.fc = nn.Linear(hidden_size, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text: [batch, seq_len]\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        # GRU output trả về: output, hidden\n",
        "        # (Khác với LSTM trả về: output, (hidden, cell))\n",
        "        output, hidden = self.gru(embedded)\n",
        "\n",
        "        # hidden shape: [num_layers, batch, hidden_size]\n",
        "        # Lấy hidden state của lớp cuối cùng (layer n)\n",
        "        last_hidden = hidden[-1, :, :]\n",
        "\n",
        "        return self.fc(last_hidden)\n",
        "\n",
        "# Khởi tạo mô hình\n",
        "model = SentimentGRU(len(word2idx), EMBEDDING_DIM, HIDDEN_SIZE, NUM_CLASSES, NUM_LAYERS).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"\\n>>> 3. Kiến trúc mô hình GRU:\")\n",
        "print(model)"
      ],
      "metadata": {
        "id": "9qtl8Wcf9VLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHẦN 4: HUẤN LUYỆN VÀ ĐÁNH GIÁ\n",
        "# ==============================================================================\n",
        "\n",
        "def compute_f1(loader, model):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for text, labels in loader:\n",
        "            text, labels = text.to(device), labels.to(device)\n",
        "            preds = model(text).argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    return f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(\"\\n>>> 4. Bắt đầu huấn luyện...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for text, labels in train_loader:\n",
        "        text, labels = text.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(text)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    val_f1 = compute_f1(val_loader, model)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {total_loss/len(train_loader):.4f} | Val F1: {val_f1:.4f}\")"
      ],
      "metadata": {
        "id": "LFfQ4bIz9gOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHẦN 5: KẾT QUẢ CUỐI CÙNG TRÊN TẬP TEST\n",
        "# ==============================================================================\n",
        "print(\"\\n>>> 5. Kết quả trên tập TEST:\")\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for text, labels in test_loader:\n",
        "        text, labels = text.to(device), labels.to(device)\n",
        "        preds = model(text).argmax(dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds, target_names=['Negative', 'Neutral', 'Positive'], digits=4))"
      ],
      "metadata": {
        "id": "OCM6Wr8K9zAb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}