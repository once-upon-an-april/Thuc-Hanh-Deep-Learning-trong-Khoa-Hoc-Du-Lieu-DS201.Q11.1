{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNaGE4S1FMAp8+uBexifJcH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/once-upon-an-april/Thuc-Hanh-Deep-Learning-trong-Khoa-Hoc-Du-Lieu-DS201.Q11.1/blob/main/Bai2/22520975_Lab3_Bai3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAxX4UZUC_lr"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BÀI 3: NHẬN DIỆN THỰC THỂ (NER) TRÊN PHO_NER_COVID19 SỬ DỤNG BILSTM\n",
        "# (5 Lớp BiLSTM, Hidden 256, F1-Score)\n",
        "# ==============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# Kiểm tra thiết bị\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHẦN 1: TẢI DỮ LIỆU TỪ GITHUB (JSONL FORMAT)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n>>> 1. Đang tải dữ liệu PhoNER_COVID19 từ GitHub...\")\n",
        "\n",
        "def load_data(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # File dạng JSON Lines (mỗi dòng là 1 json)\n",
        "        return [json.loads(line) for line in response.text.strip().split('\\n')]\n",
        "    else:\n",
        "        raise Exception(f\"Không tải được file từ {url}\")\n",
        "\n",
        "urls = {\n",
        "    \"train\": \"https://raw.githubusercontent.com/VinAIResearch/PhoNER_COVID19/master/data/word/train_word.json\",\n",
        "    \"dev\": \"https://raw.githubusercontent.com/VinAIResearch/PhoNER_COVID19/master/data/word/dev_word.json\",\n",
        "    \"test\": \"https://raw.githubusercontent.com/VinAIResearch/PhoNER_COVID19/master/data/word/test_word.json\"\n",
        "}\n",
        "\n",
        "train_data = load_data(urls['train'])\n",
        "dev_data = load_data(urls['dev'])\n",
        "test_data = load_data(urls['test'])\n",
        "\n",
        "print(f\"Đã tải xong! Train: {len(train_data)}, Dev: {len(dev_data)}, Test: {len(test_data)} câu.\")\n",
        "\n",
        "# Xem mẫu dữ liệu\n",
        "print(\"Ví dụ mẫu đầu tiên:\")\n",
        "print(f\"Words: {train_data[0]['words'][:5]}...\")\n",
        "print(f\"Tags:  {train_data[0]['tags'][:5]}...\")"
      ],
      "metadata": {
        "id": "-5xk7PgrDlKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHẦN 2: XÂY DỰNG TỪ ĐIỂN (VOCAB & TAGS)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n>>> 2. Xây dựng Vocabulary và Tag Map...\")\n",
        "\n",
        "# 2.1 Xây dựng Word Vocab\n",
        "all_words = []\n",
        "for sample in train_data:\n",
        "    all_words.extend(sample['words'])\n",
        "\n",
        "word_counts = Counter(all_words)\n",
        "# Giữ từ xuất hiện >= 2 lần\n",
        "vocab = sorted([w for w, c in word_counts.items() if c >= 2])\n",
        "\n",
        "word2idx = {w: i+2 for i, w in enumerate(vocab)}\n",
        "word2idx['<PAD>'] = 0\n",
        "word2idx['<UNK>'] = 1\n",
        "\n",
        "print(f\"Kích thước Word Vocab: {len(word2idx)}\")\n",
        "\n",
        "# 2.2 Xây dựng Tag Vocab (Nhãn thực thể)\n",
        "all_tags = []\n",
        "for sample in train_data:\n",
        "    all_tags.extend(sample['tags'])\n",
        "\n",
        "unique_tags = sorted(list(set(all_tags)))\n",
        "tag2idx = {t: i+1 for i, t in enumerate(unique_tags)}\n",
        "tag2idx['<PAD>'] = 0 # Quan trọng: Padding tag phải là 0 để ignore loss\n",
        "\n",
        "idx2tag = {i: t for t, i in tag2idx.items()}\n",
        "\n",
        "print(f\"Số lượng nhãn (Tags): {len(tag2idx)}\")\n",
        "print(f\"Danh sách nhãn: {list(tag2idx.keys())}\")"
      ],
      "metadata": {
        "id": "qCoyFm5DDtto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHẦN 3: DATASET & DATALOADER\n",
        "# ==============================================================================\n",
        "\n",
        "MAX_LEN = 100 # Độ dài cắt/pad cho câu\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "class PhoNERDataset(Dataset):\n",
        "    def __init__(self, data, word2idx, tag2idx, max_len):\n",
        "        self.data = data\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words = self.data[idx]['words']\n",
        "        tags = self.data[idx]['tags']\n",
        "\n",
        "        # Vectorize Words\n",
        "        w_idxs = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in words]\n",
        "\n",
        "        # Vectorize Tags\n",
        "        t_idxs = [self.tag2idx.get(t, 0) for t in tags] # Fallback 0 is PAD? Should ensure tags exist.\n",
        "\n",
        "        # Padding / Truncate\n",
        "        if len(w_idxs) < self.max_len:\n",
        "            # Pad\n",
        "            w_idxs = w_idxs + [self.word2idx['<PAD>']] * (self.max_len - len(w_idxs))\n",
        "            t_idxs = t_idxs + [self.tag2idx['<PAD>']] * (self.max_len - len(t_idxs))\n",
        "        else:\n",
        "            # Truncate\n",
        "            w_idxs = w_idxs[:self.max_len]\n",
        "            t_idxs = t_idxs[:self.max_len]\n",
        "\n",
        "        return torch.tensor(w_idxs, dtype=torch.long), torch.tensor(t_idxs, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(PhoNERDataset(train_data, word2idx, tag2idx, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_loader = DataLoader(PhoNERDataset(dev_data, word2idx, tag2idx, MAX_LEN), batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(PhoNERDataset(test_data, word2idx, tag2idx, MAX_LEN), batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "xtecm9BxDwlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHẦN 4: MÔ HÌNH BiLSTM CHO NER\n",
        "# ==============================================================================\n",
        "\n",
        "class BiLSTM_NER(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_dim, n_layers):\n",
        "        super(BiLSTM_NER, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # Bi-LSTM: bidirectional=True\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True, # QUAN TRỌNG\n",
        "            dropout=0.3\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        # Vì là BiLSTM nên hidden state đầu ra sẽ gấp đôi (hidden_size * 2)\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text: [batch, seq_len]\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        # output: [batch, seq_len, hidden_size * 2]\n",
        "        output, _ = self.lstm(embedded)\n",
        "\n",
        "        # Đưa qua lớp Linear để phân loại từng token\n",
        "        # predictions: [batch, seq_len, output_dim]\n",
        "        predictions = self.fc(output)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Khởi tạo mô hình\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_SIZE = 256 # Theo đề bài\n",
        "NUM_LAYERS = 5    # Theo đề bài\n",
        "OUTPUT_DIM = len(tag2idx) # Số lượng nhãn BIO\n",
        "\n",
        "model = BiLSTM_NER(len(word2idx), EMBEDDING_DIM, HIDDEN_SIZE, OUTPUT_DIM, NUM_LAYERS)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function: Cần ignore index 0 (PAD) để không tính loss cho phần đệm\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tag2idx['<PAD>'])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"\\n>>> 3. Kiến trúc mô hình BiLSTM-NER:\")\n",
        "print(model)"
      ],
      "metadata": {
        "id": "R1TVCt1ND4kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHẦN 5: HUẤN LUYỆN\n",
        "# ==============================================================================\n",
        "\n",
        "# Hàm tính F1 Token-level (Bỏ qua PAD)\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text, tags in loader:\n",
        "            text, tags = text.to(device), tags.to(device)\n",
        "\n",
        "            # Forward\n",
        "            predictions = model(text) # [batch, seq_len, num_tags]\n",
        "\n",
        "            # Lấy argmax để ra nhãn dự đoán\n",
        "            preds = torch.argmax(predictions, dim=2) # [batch, seq_len]\n",
        "\n",
        "            # Flatten để tính metrics, nhưng cần lọc bỏ PAD\n",
        "            # Chuyển về CPU numpy\n",
        "            preds = preds.cpu().numpy().flatten()\n",
        "            tags = tags.cpu().numpy().flatten()\n",
        "\n",
        "            # Lọc mask: Chỉ lấy những vị trí tag != PAD\n",
        "            mask = (tags != tag2idx['<PAD>'])\n",
        "\n",
        "            valid_preds = preds[mask]\n",
        "            valid_tags = tags[mask]\n",
        "\n",
        "            all_preds.extend(valid_preds)\n",
        "            all_labels.extend(valid_tags)\n",
        "\n",
        "    return f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(\"\\n>>> 4. Bắt đầu huấn luyện...\")\n",
        "EPOCHS = 5 # Demo chạy 5 epochs\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for text, tags in train_loader:\n",
        "        text, tags = text.to(device), tags.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(text)\n",
        "\n",
        "        # Reshape để tính Loss\n",
        "        # predictions: [batch * seq_len, num_tags]\n",
        "        # tags: [batch * seq_len]\n",
        "        loss = criterion(predictions.view(-1, OUTPUT_DIM), tags.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    val_f1 = evaluate(model, dev_loader)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {epoch_loss/len(train_loader):.4f} | Val F1: {val_f1:.4f}\")"
      ],
      "metadata": {
        "id": "fvWZS2KDD5Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHẦN 6: ĐÁNH GIÁ TRÊN TEST\n",
        "# ==============================================================================\n",
        "print(\"\\n>>> 5. Kết quả chi tiết trên tập TEST:\")\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for text, tags in test_loader:\n",
        "        text, tags = text.to(device), tags.to(device)\n",
        "        predictions = model(text)\n",
        "        preds = torch.argmax(predictions, dim=2)\n",
        "\n",
        "        preds = preds.cpu().numpy().flatten()\n",
        "        tags = tags.cpu().numpy().flatten()\n",
        "\n",
        "        # Lọc bỏ PAD (Chỉ đánh giá trên các token thực)\n",
        "        mask = (tags != tag2idx['<PAD>'])\n",
        "        all_preds.extend(preds[mask])\n",
        "        all_labels.extend(tags[mask])\n",
        "\n",
        "# Map từ index về tên nhãn để in report cho dễ hiểu\n",
        "# Lấy tập hợp tất cả các nhãn xuất hiện trong dữ liệu thực tế VÀ dự đoán\n",
        "present_labels = sorted(list(set(all_labels) | set(all_preds)))\n",
        "target_names = [idx2tag.get(i, f\"UNK_{i}\") for i in present_labels]\n",
        "\n",
        "# zero_division=0: Gán điểm 0 cho các nhãn model không dự đoán được thay vì báo lỗi/warning\n",
        "print(classification_report(all_labels, all_preds, labels=present_labels, target_names=target_names, digits=4, zero_division=0))"
      ],
      "metadata": {
        "id": "yHHC-77yD-ta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}