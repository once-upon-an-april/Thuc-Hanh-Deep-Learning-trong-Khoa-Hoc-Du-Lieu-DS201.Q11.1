{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO1E9SBdyrh63CKFAfO9BlZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/once-upon-an-april/Thuc-Hanh-Deep-Learning-trong-Khoa-Hoc-Du-Lieu-DS201.Q11.1/blob/main/Bai2/22520975_Lab3_Bai1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isZIGHYH16sN"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BÀI 1: PHÂN LOẠI CẢM XÚC TRÊN DATASET UIT-VSFC SỬ DỤNG LSTM (5 LỚP, HIDDEN 256)\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. CÀI ĐẶT THƯ VIỆN CẦN THIẾT\n",
        "!pip install pyvi datasets torch scikit-learn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "from datasets import load_dataset\n",
        "from pyvi import ViTokenizer\n",
        "\n",
        "# Kiểm tra GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHẦN 1: TẢI VÀ TIỀN XỬ LÝ DỮ LIỆU (PREPROCESSING)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n>>> 1. Đang tải dataset UIT-VSFC...\")\n",
        "# Tải phiên bản Parquet để tránh lỗi script bảo mật\n",
        "try:\n",
        "    ds = load_dataset(\"uitnlp/vietnamese_students_feedback\", name=\"default\", revision=\"refs/convert/parquet\")\n",
        "except Exception:\n",
        "    # Fallback nếu mạng lỗi\n",
        "    ds = load_dataset(\"uitnlp/vietnamese_students_feedback\", trust_remote_code=True)\n",
        "\n",
        "# Chuyển sang Pandas DataFrame\n",
        "train_df = pd.DataFrame(ds['train'])\n",
        "val_df = pd.DataFrame(ds['validation'])\n",
        "test_df = pd.DataFrame(ds['test'])\n",
        "\n",
        "# --- HÀM TIỀN XỬ LÝ ---\n",
        "def preprocess_text(text):\n",
        "    # 1. Chuyển về chữ thường\n",
        "    text = text.lower()\n",
        "    # 2. Loại bỏ dấu câu (giữ lại tiếng Việt và số)\n",
        "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
        "    # 3. Tách từ tiếng Việt\n",
        "    text = ViTokenizer.tokenize(text)\n",
        "    return text\n",
        "\n",
        "print(\">>> Đang thực hiện Preprocessing (Clean & Tokenize)...\")\n",
        "train_df['sentence_cleaned'] = train_df['sentence'].astype(str).apply(preprocess_text)\n",
        "val_df['sentence_cleaned'] = val_df['sentence'].astype(str).apply(preprocess_text)\n",
        "test_df['sentence_cleaned'] = test_df['sentence'].astype(str).apply(preprocess_text)\n",
        "\n",
        "print(\"Mẫu dữ liệu sau khi làm sạch:\")\n",
        "print(train_df[['sentence', 'sentence_cleaned']].head(2))\n"
      ],
      "metadata": {
        "id": "ihiWrVm13Q_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHẦN 2: XÂY DỰNG TỪ ĐIỂN VÀ DATASET CHO LSTM\n",
        "# (Thay thế TF-IDF bằng Word Indexing cho mô hình Sequence)\n",
        "# ==============================================================================\n",
        "\n",
        "# Cấu hình Hyperparameters\n",
        "MAX_LEN = 50       # Độ dài tối đa của câu (padding/truncate)\n",
        "BATCH_SIZE = 64    # Kích thước batch\n",
        "EMBEDDING_DIM = 100 # Kích thước vector biểu diễn từ\n",
        "HIDDEN_SIZE = 256  # Yêu cầu đề bài\n",
        "NUM_LAYERS = 5     # Yêu cầu đề bài: 5 lớp LSTM\n",
        "NUM_CLASSES = 3    # Negative, Neutral, Positive\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 10\n",
        "\n",
        "# 1. Xây dựng bộ từ điển (Vocabulary)\n",
        "print(\"\\n>>> 2. Xây dựng Vocabulary...\")\n",
        "all_words = \" \".join(train_df['sentence_cleaned']).split()\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "# Chỉ giữ lại các từ xuất hiện ít nhất 2 lần để giảm nhiễu\n",
        "vocab = sorted([w for w, c in word_counts.items() if c >= 2])\n",
        "word2idx = {w: i+2 for i, w in enumerate(vocab)} # i+2 vì 0 dành cho padding, 1 cho unknown\n",
        "word2idx['<PAD>'] = 0\n",
        "word2idx['<UNK>'] = 1\n",
        "\n",
        "vocab_size = len(word2idx)\n",
        "print(f\"Kích thước từ điển (Vocab Size): {vocab_size}\")\n",
        "\n",
        "# 2. Hàm chuyển câu văn bản thành chuỗi số (Vectorize)\n",
        "def vectorize_text(text, word2idx, max_len):\n",
        "    words = text.split()\n",
        "    # Chuyển từ thành index, nếu không có thì dùng <UNK>\n",
        "    idxs = [word2idx.get(w, word2idx['<UNK>']) for w in words]\n",
        "\n",
        "    # Padding hoặc Truncate\n",
        "    if len(idxs) < max_len:\n",
        "        idxs = idxs + [word2idx['<PAD>']] * (max_len - len(idxs))\n",
        "    else:\n",
        "        idxs = idxs[:max_len]\n",
        "    return idxs\n",
        "\n",
        "# 3. Tạo Custom Dataset cho PyTorch\n",
        "class VSFC_Dataset(Dataset):\n",
        "    def __init__(self, df, word2idx, max_len):\n",
        "        self.sentences = df['sentence_cleaned'].values\n",
        "        self.labels = df['sentiment'].values\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Vector hóa\n",
        "        vector = vectorize_text(text, self.word2idx, self.max_len)\n",
        "\n",
        "        return torch.tensor(vector, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Tạo DataLoader\n",
        "train_dataset = VSFC_Dataset(train_df, word2idx, MAX_LEN)\n",
        "val_dataset = VSFC_Dataset(val_df, word2idx, MAX_LEN)\n",
        "test_dataset = VSFC_Dataset(test_df, word2idx, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "yKNjmCFR3ZEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHẦN 3: ĐỊNH NGHĨA MÔ HÌNH LSTM (5 LỚP, HIDDEN 256)\n",
        "# ==============================================================================\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_dim, n_layers):\n",
        "        super(SentimentLSTM, self).__init__()\n",
        "\n",
        "        # 1. Embedding Layer: Chuyển index thành vector dày đặc\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # 2. LSTM Layers\n",
        "        # input_size = embedding_dim\n",
        "        # batch_first=True: input shape là (batch, seq_len, features)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.3  # Thêm dropout để tránh Overfitting vì model khá sâu (5 lớp)\n",
        "        )\n",
        "\n",
        "        # 3. Fully Connected Layer (Output)\n",
        "        self.fc = nn.Linear(hidden_size, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text shape: [batch size, sent len]\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded shape: [batch size, sent len, emb dim]\n",
        "\n",
        "        # LSTM output\n",
        "        # output: chứa hidden state của tất cả time steps\n",
        "        # (hidden, cell): trạng thái ẩn cuối cùng\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        # Lấy hidden state của lớp cuối cùng (layer cuối, time step cuối)\n",
        "        # hidden shape: [num_layers, batch size, hidden dim]\n",
        "        # Ta lấy lớp cuối cùng: hidden[-1, :, :]\n",
        "        last_hidden = hidden[-1, :, :]\n",
        "\n",
        "        return self.fc(last_hidden)\n",
        "\n",
        "# Khởi tạo mô hình\n",
        "model = SentimentLSTM(vocab_size, EMBEDDING_DIM, HIDDEN_SIZE, NUM_CLASSES, NUM_LAYERS)\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"\\n>>> 3. Kiến trúc mô hình:\")\n",
        "print(model)\n",
        "\n",
        "# Hàm Loss và Optimizer (Adam)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "_ftnwQgJ3g_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHẦN 4: HUẤN LUYỆN VÀ ĐÁNH GIÁ (TRAINING LOOP)\n",
        "# ==============================================================================\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    # Average='weighted' phù hợp cho dataset mất cân bằng\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    return f1\n",
        "\n",
        "print(\"\\n>>> 4. Bắt đầu huấn luyện...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for text, labels in train_loader:\n",
        "        text, labels = text.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(text)\n",
        "\n",
        "        loss = criterion(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Lưu lại để tính metrics\n",
        "        preds = torch.argmax(predictions, dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_f1 = calculate_metrics(all_labels, all_preds)\n",
        "\n",
        "    # --- Validation ---\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text, labels in val_loader:\n",
        "            text, labels = text.to(device), labels.to(device)\n",
        "            predictions = model(text)\n",
        "            loss = criterion(predictions, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(predictions, dim=1).cpu().numpy()\n",
        "            val_preds.extend(preds)\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_f1 = calculate_metrics(val_labels, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
        "          f\"Train Loss: {train_loss/len(train_loader):.4f} | Train F1: {train_f1:.4f} | \"\n",
        "          f\"Val Loss: {val_loss/len(val_loader):.4f} | Val F1: {val_f1:.4f}\")"
      ],
      "metadata": {
        "id": "6NBM-jtA3k4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHẦN 5: ĐÁNH GIÁ TRÊN TẬP TEST\n",
        "# ==============================================================================\n",
        "print(\"\\n>>> 5. Đánh giá kết quả trên tập TEST...\")\n",
        "model.eval()\n",
        "test_preds = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for text, labels in test_loader:\n",
        "        text, labels = text.to(device), labels.to(device)\n",
        "        predictions = model(text)\n",
        "        preds = torch.argmax(predictions, dim=1).cpu().numpy()\n",
        "        test_preds.extend(preds)\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# In báo cáo chi tiết\n",
        "print(\"\\nClassification Report (Test Set):\")\n",
        "target_names = ['Negative', 'Neutral', 'Positive']\n",
        "print(classification_report(test_labels, test_preds, target_names=target_names, digits=4))\n",
        "\n",
        "f1_final = f1_score(test_labels, test_preds, average='weighted')\n",
        "print(f\"Final Weighted F1-Score: {f1_final:.4f}\")"
      ],
      "metadata": {
        "id": "qbNYmB3C3pgw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}