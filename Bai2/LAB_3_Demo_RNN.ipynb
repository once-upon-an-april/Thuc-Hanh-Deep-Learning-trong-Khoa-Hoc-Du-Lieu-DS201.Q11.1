{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/once-upon-an-april/Thuc-Hanh-Deep-Learning-trong-Khoa-Hoc-Du-Lieu-DS201.Q11.1/blob/main/Bai2/LAB_3_Demo_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN đơn giản (one-to-one, many-to-one)"
      ],
      "metadata": {
        "id": "Ne25JihZKC-K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7rDoDcRJ8-6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Cấu hình mô hình RNN nhiều lớp (many-to-one)\n",
        "num_classes = 5  # ví dụ 5 lớp phân loại\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=32),   # Embedding từ từ thành vector\n",
        "    tf.keras.layers.SimpleRNN(64),                              # RNN layer với 64 đơn vị ẩn\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')    # Lớp output với softmax\n",
        "])\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "Yvx0iac7KXMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Ví dụ LSTM cho phân loại chuỗi (many-to-one)\n",
        "num_classes = 3  # ví dụ 3 lớp phân loại\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),   # Embedding với 64 chiều\n",
        "    tf.keras.layers.LSTM(128),                                  # LSTM layer với 128 đơn vị ẩn\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')    # Output softmax\n",
        "])\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "FBFtOFxDKGho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU"
      ],
      "metadata": {
        "id": "VVQK49N_Kcd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Ví dụ GRU cho phân loại chuỗi\n",
        "num_classes = 4\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),   # Embedding\n",
        "    tf.keras.layers.GRU(64),                                    # GRU layer với 64 đơn vị ẩn\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')    # Output softmax\n",
        "])\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "moYFIqYlKfH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BiLSTM"
      ],
      "metadata": {
        "id": "qPGx3YAcKh7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Ví dụ BiLSTM cho phân loại chuỗi (many-to-one)\n",
        "num_classes = 2\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),     # Embedding\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),      # BiLSTM với 64 đơn vị (32 mỗi chiều)\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "ceuwniaqKkwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mạng LSTM xếp chồng"
      ],
      "metadata": {
        "id": "v3C_n-t7Km_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Ví dụ Stacked LSTM (2 tầng)\n",
        "num_classes = 5\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),\n",
        "    tf.keras.layers.LSTM(128, return_sequences=True),  # LSTM tầng 1 (xuất ra chuỗi)\n",
        "    tf.keras.layers.LSTM(128),                         # LSTM tầng 2 (xuất ra ẩn cuối)\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "GZayHaZjKpN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline xử lý văn bản (Phân loại)"
      ],
      "metadata": {
        "id": "5x-CTayuKvqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. Dữ liệu văn bản mẫu\n",
        "texts = [\n",
        "    \"Tôi yêu học máy\",\n",
        "    \"Mạng nơ-ron rất mạnh mẽ\",\n",
        "    \"Học sâu trong NLP\"\n",
        "]\n",
        "labels = [1, 0, 1]  # Ví dụ nhãn nhị phân\n",
        "\n",
        "# 2. Tokenization & tạo từ điển\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# 3. Đệm chuỗi về độ dài bằng nhau\n",
        "data = pad_sequences(sequences, maxlen=5)\n",
        "print(data)  # Mỗi câu được chuyển thành mảng số nguyên cố định độ dài"
      ],
      "metadata": {
        "id": "se9dAoWAKrm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nhận dạng Thực thể có tên (NER) với BiLSTM"
      ],
      "metadata": {
        "id": "hGCqb8vbK2fP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Giả sử có 3 loại nhãn (ví dụ: PERSON, ORG, O)\n",
        "num_tags = 3\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),\n",
        "    tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(64, return_sequences=True)\n",
        "    ),\n",
        "    tf.keras.layers.TimeDistributed(\n",
        "        tf.keras.layers.Dense(num_tags, activation='softmax')\n",
        "    )\n",
        "])\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "DyoY5yAIKyxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA DATASET\n",
        "\n",
        "Yêu cầu: Thực hiện EDA và tiền xử lý 2 bộ dataset trước khi làm bài lab"
      ],
      "metadata": {
        "id": "J5WW4XnQRGnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BỘ DỮ LIỆU UIT-VSFC (Phân loại văn bản)"
      ],
      "metadata": {
        "id": "mdmujq8ARYSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Kiểm tra cấu trúc bộ dữ liệu\n",
        "\n",
        "* Tải và đọc các file .txt hoặc .csv từ UIT-VSFC.\n",
        "\n",
        "* Kiểm tra các cột có trong dataset (ví dụ: sentence, label).\n",
        "\n",
        "* Kiểm tra số lượng mẫu train/valid/test.\n",
        "\n",
        "* Lấy 10 mẫu đầu tiên để xem dữ liệu thực trông như thế nào."
      ],
      "metadata": {
        "id": "-rVMu2fsRgXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# Tải bộ dữ liệu UIT-VSFC\n",
        "# 1. Tải dataset\n",
        "print(\">>> Đang tải dataset (phiên bản Parquet)...\")\n",
        "try:\n",
        "    ds = load_dataset(\n",
        "        \"uitnlp/vietnamese_students_feedback\",\n",
        "        name=\"default\", # Cấu hình mặc định\n",
        "        revision=\"refs/convert/parquet\" # Chỉ định nhánh chứa file parquet\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"Lỗi khi tải trực tiếp, chuyển sang phương án tải thủ công (bên dưới).\")\n",
        "    raise e\n",
        "\n",
        "# Kiểm tra các cột có trong dataset\n",
        "print(\"=\"*30)\n",
        "print(\"CẤU TRÚC DATASET\")\n",
        "print(\"=\"*30)\n",
        "print(f\"Các cột trong tập Train: {ds['train'].column_names}\")\n",
        "print(\"Chi tiết Features:\")\n",
        "print(ds['train'].features)\n",
        "\n",
        "# 3. Kiểm tra số lượng mẫu\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"SỐ LƯỢNG MẪU\")\n",
        "print(\"=\"*30)\n",
        "print(f\"- Train set:      {len(ds['train'])} mẫu\")\n",
        "print(f\"- Validation set: {len(ds['validation'])} mẫu\")\n",
        "print(f\"- Test set:       {len(ds['test'])} mẫu\")\n",
        "print(f\"-> Tổng cộng:     {len(ds['train']) + len(ds['validation']) + len(ds['test'])} mẫu\")\n",
        "\n",
        "# 4. Xem 10 mẫu dữ liệu thực tế đầu tiên\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"10 MẪU DỮ LIỆU ĐẦU TIÊN (Tập Train)\")\n",
        "print(\"=\"*30)"
      ],
      "metadata": {
        "id": "nnu_m_luRcQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Phân tích phân bố nhãn (Label Distribution)\n",
        "\n",
        "* Thống kê tần suất mỗi nhãn (Positive / Negative / Neutral).\n",
        "\n",
        "* Vẽ biểu đồ (bar chart) phân bố nhãn.\n",
        "\n",
        "* **Trả lời câu hỏi**: Bộ dữ liệu có bị mất cân bằng nhãn không?"
      ],
      "metadata": {
        "id": "bztHZ_s9Ro-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Chuyển tập TRAIN sang DataFrame để dễ tính toán\n",
        "df_train = pd.DataFrame(ds['train'])\n",
        "\n",
        "# 2. Thống kê tần suất mỗi nhãn\n",
        "# Map từ số sang chữ cho dễ hiểu: 0: Negative, 1: Neutral, 2: Positive\n",
        "label_map = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
        "df_train['label_name'] = df_train['sentiment'].map(label_map)\n",
        "\n",
        "# Đếm số lượng và sắp xếp theo thứ tự logic: Tiêu cực -> Trung tính -> Tích cực\n",
        "counts = df_train['label_name'].value_counts()\n",
        "desired_order = ['Negative', 'Neutral', 'Positive']\n",
        "counts = counts.reindex(desired_order)\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"THỐNG KÊ PHÂN BỐ NHÃN (Tập Train)\")\n",
        "print(\"=\"*30)\n",
        "print(counts)\n",
        "\n",
        "# Tính tỷ lệ phần trăm\n",
        "percentages = (counts / len(df_train)) * 100\n",
        "print(\"\\nTỷ lệ phần trăm:\")\n",
        "print(percentages.map('{:.2f}%'.format))\n",
        "\n",
        "# 3. Vẽ biểu đồ Bar Chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "colors = ['#d62728', '#7f7f7f', '#2ca02c'] # Đỏ (Neg), Xám (Neu), Xanh (Pos)\n",
        "bars = plt.bar(counts.index, counts.values, color=colors)\n",
        "\n",
        "plt.title('Phân bố nhãn cảm xúc (Sentiment) - Tập Train')\n",
        "plt.xlabel('Nhãn (Label)')\n",
        "plt.ylabel('Số lượng mẫu')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Hiển thị con số cụ thể trên đầu mỗi cột\n",
        "plt.bar_label(bars, fmt='%d')\n",
        "plt.show()\n",
        "\n",
        "# 4. Trả lời câu hỏi: Có mất cân bằng không?\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"KẾT LUẬN VỀ MẤT CÂN BẰNG\")\n",
        "print(\"=\"*30)\n",
        "neutral_percent = percentages['Neutral']\n",
        "if neutral_percent < 15: # Ngưỡng thường dùng để xác định mất cân bằng nặng\n",
        "    print(f\"-> CÓ. Bộ dữ liệu bị mất cân bằng nặng.\")\n",
        "    print(f\"-> Lý do: Nhãn 'Neutral' chỉ chiếm {neutral_percent:.2f}%, thấp hơn rất nhiều so với Negative và Positive.\")\n",
        "    print(\"-> Khuyến nghị: Cần dùng kỹ thuật Oversampling, Class Weighting hoặc Stratified Split khi train.\")\n",
        "else:\n",
        "    print(\"-> Bộ dữ liệu tương đối cân bằng.\")"
      ],
      "metadata": {
        "id": "QB9Qn7M_R1Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Phân tích độ dài câu\n",
        "\n",
        "* Tính số token/word trong từng câu.\n",
        "\n",
        "* Vẽ histogram hoặc boxplot độ dài câu.\n",
        "\n",
        "Tìm: Max sequence length hợp lý khi padding (ví dụ: 50, 100, 200)."
      ],
      "metadata": {
        "id": "DZ5v-qqUR132"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# --- PHẦN 3: PHÂN TÍCH ĐỘ DÀI CÂU ---\n",
        "\n",
        "# 1. Tính số từ (tokens) trong mỗi câu\n",
        "# Lưu ý: Ở đây tách theo khoảng trắng (space) để ước lượng nhanh.\n",
        "# Nếu làm kỹ hơn cho tiếng Việt, bạn có thể dùng pyvi hoặc VnCoreNLP.\n",
        "df_train['num_tokens'] = df_train['sentence'].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "# 2. Thống kê cơ bản\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"THỐNG KÊ ĐỘ DÀI CÂU\")\n",
        "print(\"=\"*30)\n",
        "print(df_train['num_tokens'].describe())\n",
        "\n",
        "# 3. Vẽ biểu đồ (Histogram & Boxplot)\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Histogram: Xem phân phối chung\n",
        "sns.histplot(df_train['num_tokens'], bins=30, kde=True, color='skyblue', ax=ax[0])\n",
        "ax[0].set_title('Phân phối độ dài câu (Histogram)')\n",
        "ax[0].set_xlabel('Số lượng từ')\n",
        "ax[0].set_ylabel('Số lượng câu')\n",
        "\n",
        "# Boxplot: Xem outlier (câu quá dài)\n",
        "sns.boxplot(x=df_train['num_tokens'], color='lightgreen', ax=ax[1])\n",
        "ax[1].set_title('Biểu đồ hộp (Boxplot) xác định Outliers')\n",
        "ax[1].set_xlabel('Số lượng từ')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Tìm Max Sequence Length hợp lý\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"CHỌN MAX SEQUENCE LENGTH (PADDING)\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Tính các mốc phần trăm (Quantiles)\n",
        "quantile_90 = df_train['num_tokens'].quantile(0.90)\n",
        "quantile_95 = df_train['num_tokens'].quantile(0.95)\n",
        "quantile_99 = df_train['num_tokens'].quantile(0.99)\n",
        "max_len_real = df_train['num_tokens'].max()\n",
        "\n",
        "print(f\"- Độ dài bao phủ 90% dữ liệu: {int(quantile_90)} từ\")\n",
        "print(f\"- Độ dài bao phủ 95% dữ liệu: {int(quantile_95)} từ\")\n",
        "print(f\"- Độ dài bao phủ 99% dữ liệu: {int(quantile_99)} từ\")\n",
        "print(f\"- Câu dài nhất thực tế:       {max_len_real} từ\")\n",
        "\n",
        "# Đề xuất\n",
        "suggested_len = int(quantile_95)\n",
        "print(f\"\\n-> KHUYẾN NGHỊ: Nên chọn max_len khoảng {suggested_len} - {suggested_len + 5}.\")\n",
        "print(f\"   (Lý do: Bao phủ được 95% dữ liệu, tránh lãng phí bộ nhớ do padding quá nhiều cho các câu outlier dài {max_len_real} từ).\")"
      ],
      "metadata": {
        "id": "8qsdzUqaSCY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Thống kê từ vựng\n",
        "\n",
        "* Đếm số lượng từ vựng (unique tokens).\n",
        "\n",
        "* Top 20 từ xuất hiện nhiều nhất.\n",
        "\n",
        "* Trả lời câu hỏi: Có cần làm preprocessing không? (lowercase, remove punctuation, fix unicode…)"
      ],
      "metadata": {
        "id": "VRyLFCtZSO3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# --- PHẦN 4: THỐNG KÊ TỪ VỰNG (VOCABULARY) ---\n",
        "\n",
        "# 1. Tạo danh sách tất cả các token (tách đơn giản bằng khoảng trắng để kiểm tra thô)\n",
        "# Lưu ý: Đây chưa phải là word segmentation chuẩn tiếng Việt, chỉ để kiểm tra độ sạch của dữ liệu.\n",
        "all_words = \" \".join(df_train['sentence'].astype(str)).split()\n",
        "\n",
        "# 2. Đếm số lượng từ vựng (Unique tokens)\n",
        "word_counts = Counter(all_words)\n",
        "vocab_size = len(word_counts)\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"THỐNG KÊ TỪ VỰNG (Raw Data)\")\n",
        "print(\"=\"*30)\n",
        "print(f\"Tổng số từ (Total tokens):   {len(all_words)}\")\n",
        "print(f\"Kích thước từ điển (Vocab):  {vocab_size} từ (unique)\")\n",
        "\n",
        "# 3. Top 20 từ xuất hiện nhiều nhất\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"TOP 20 TỪ XUẤT HIỆN NHIỀU NHẤT\")\n",
        "print(\"=\"*30)\n",
        "top_20 = word_counts.most_common(20)\n",
        "\n",
        "# Hiển thị dạng bảng cho dễ nhìn\n",
        "df_top20 = pd.DataFrame(top_20, columns=['Từ (Token)', 'Số lần xuất hiện'])\n",
        "print(df_top20)\n",
        "\n",
        "# 4. Kiểm tra nhanh một số trường hợp cụ thể để quyết định preprocessing\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"KIỂM TRA NHU CẦU PREPROCESSING\")\n",
        "print(\"=\"*30)\n",
        "print(f\"- 'thầy' xuất hiện: {word_counts['thầy']} lần\")\n",
        "print(f\"- 'Thầy' xuất hiện: {word_counts['Thầy']} lần (Case sensitive?)\")\n",
        "print(f\"- 'tốt'  xuất hiện: {word_counts['tốt']} lần\")\n",
        "print(f\"- 'tốt.' xuất hiện: {word_counts['tốt.']} lần (Dính dấu câu?)\")\n",
        "print(f\"- 'ko'   xuất hiện: {word_counts['ko']} lần (Teencode?)\")"
      ],
      "metadata": {
        "id": "E3czyk7bSI18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Tiền xử lý dữ liệu\n",
        "\n",
        "* Sinh viên phải mô tả:\n",
        "\n",
        "* Các bước preprocessing sẽ áp dụng cho text: chuẩn hóa unicode, tách từ (tokenization dùng VnCoreNLP / underthesea / pyvi…), lowercasing, xử lý emoji, loại bỏ ký tự thừa, ...\n",
        "\n",
        "Minh hoạ bằng ví dụ before → after."
      ],
      "metadata": {
        "id": "gT2ykxZ1SRFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi\n",
        "\n",
        "import re\n",
        "from pyvi import ViTokenizer\n",
        "import string\n",
        "\n",
        "# Hàm tiền xử lý toàn diện\n",
        "def preprocess_text(text):\n",
        "    # 1. Chuyển về chữ thường (Lowercase)\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Loại bỏ các ký tự đặc biệt, dấu câu không cần thiết\n",
        "    # Giữ lại các từ tiếng Việt và số, loại bỏ emoji hoặc ký tự lạ\n",
        "    # Cách đơn giản: Xóa các ký tự trong chuỗi string.punctuation\n",
        "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
        "\n",
        "    # 3. Tách từ tiếng Việt (Word Segmentation) -> QUAN TRỌNG NHẤT\n",
        "    # Ví dụ: \"sinh viên\" -> \"sinh_viên\"\n",
        "    text = ViTokenizer.tokenize(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# --- ÁP DỤNG VÀO DATAFRAME ---\n",
        "print(\">>> Đang thực hiện Preprocessing...\")\n",
        "\n",
        "# Tạo cột mới 'sentence_cleaned' để giữ lại dữ liệu gốc đối chiếu\n",
        "df_train['sentence_cleaned'] = df_train['sentence'].astype(str).apply(preprocess_text)\n",
        "\n",
        "# Xem kết quả so sánh\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"SO SÁNH TRƯỚC VÀ SAU KHI XỬ LÝ\")\n",
        "print(\"=\"*30)\n",
        "print(df_train[['sentence', 'sentence_cleaned']].head(5))\n",
        "\n",
        "# Kiểm tra lại Top từ vựng sau khi xử lý\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"TOP 10 TỪ VỰNG SAU KHI TÁCH TỪ (Preprocessed)\")\n",
        "print(\"=\"*30)\n",
        "all_words_clean = \" \".join(df_train['sentence_cleaned']).split()\n",
        "print(pd.Series(all_words_clean).value_counts().head(10))"
      ],
      "metadata": {
        "id": "8hxu0XU9SNSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chuyển đổi Valid và Test sang DataFrame\n",
        "df_valid = pd.DataFrame(ds['validation'])\n",
        "df_test = pd.DataFrame(ds['test'])\n",
        "\n",
        "print(\">>> Đang xử lý tập Validation và Test...\")\n",
        "# Áp dụng hàm preprocess_text đã viết ở bước trước\n",
        "df_valid['sentence_cleaned'] = df_valid['sentence'].astype(str).apply(preprocess_text)\n",
        "df_test['sentence_cleaned'] = df_test['sentence'].astype(str).apply(preprocess_text)\n",
        "\n",
        "print(\"Đã xử lý xong toàn bộ dữ liệu!\")"
      ],
      "metadata": {
        "id": "Udun1DuWVZdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Gom toàn bộ văn bản sạch lại thành 1 chuỗi dài\n",
        "all_text = \" \".join(df_train['sentence_cleaned'])\n",
        "\n",
        "# Tạo WordCloud\n",
        "wordcloud = WordCloud(\n",
        "    width=800,\n",
        "    height=400,\n",
        "    background_color='white',\n",
        "    colormap='viridis',\n",
        "    max_words=100\n",
        ").generate(all_text)\n",
        "\n",
        "# Vẽ biểu đồ\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('WordCloud - Các từ phổ biến nhất trong UIT-VSFC')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6pUTGbB_VevG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"VECTOR HÓA DỮ LIỆU (TF-IDF)\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Cấu hình TF-IDF\n",
        "# max_features=5000: Chỉ giữ lại 5000 từ quan trọng nhất để giảm nhiễu và nhẹ máy\n",
        "# ngram_range=(1, 1): Chỉ lấy từ đơn (vì đã ghép từ bằng pyvi rồi nên không cần ngram 2-3 nữa)\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Học từ vựng từ tập Train và biến đổi dữ liệu\n",
        "X_train_tfidf = tfidf.fit_transform(df_train['sentence_cleaned'])\n",
        "\n",
        "# Chỉ biến đổi tập Valid/Test (KHÔNG fit lại để tránh data leakage)\n",
        "X_valid_tfidf = tfidf.transform(df_valid['sentence_cleaned'])\n",
        "X_test_tfidf = tfidf.transform(df_test['sentence_cleaned'])\n",
        "\n",
        "# Lấy nhãn (y)\n",
        "y_train = df_train['sentiment']\n",
        "y_valid = df_valid['sentiment']\n",
        "y_test = df_test['sentiment']\n",
        "\n",
        "print(f\"Kích thước tập Train sau khi vector hóa: {X_train_tfidf.shape}\")\n",
        "print(f\"Kích thước tập Test sau khi vector hóa:  {X_test_tfidf.shape}\")\n",
        "print(\"-\" * 30)\n",
        "print(\"Ví dụ: Từ vựng index 100 đến 110 trong từ điển:\")\n",
        "print(list(tfidf.vocabulary_.keys())[100:110])"
      ],
      "metadata": {
        "id": "XxJkf3mjVhRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BỘ DỮ LIỆU PhoNERT"
      ],
      "metadata": {
        "id": "n5XhFjm4SNvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Kiểm tra cấu trúc dữ liệu\n",
        "\n",
        "* Tải và đọc các file train, test, dev.\n",
        "\n",
        "* In 20 dòng đầu để xem cấu trúc dữ liệu BIO"
      ],
      "metadata": {
        "id": "d8D42gMiSqtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# 1. Tải dữ liệu từ GitHub VinAIResearch\n",
        "urls = {\n",
        "    \"train\": \"https://raw.githubusercontent.com/VinAIResearch/PhoNER_COVID19/master/data/word/train_word.json\",\n",
        "    \"dev\": \"https://raw.githubusercontent.com/VinAIResearch/PhoNER_COVID19/master/data/word/dev_word.json\",\n",
        "    \"test\": \"https://raw.githubusercontent.com/VinAIResearch/PhoNER_COVID19/master/data/word/test_word.json\"\n",
        "}\n",
        "\n",
        "print(\">>> Đang tải dữ liệu từ GitHub VinAIResearch (JSONL Format)...\")\n",
        "data = {}\n",
        "\n",
        "for split, url in urls.items():\n",
        "    print(f\"Downloading {split}...\")\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # --- SỬA LỖI TẠI ĐÂY ---\n",
        "        # File dạng JSONL: Tách từng dòng ra và load json cho từng dòng\n",
        "        lines = response.text.strip().split('\\n')\n",
        "        data[split] = [json.loads(line) for line in lines]\n",
        "    else:\n",
        "        print(f\"Lỗi tải file {split}: {response.status_code}\")\n",
        "\n",
        "# 2. Kiểm tra cấu trúc chung\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"CẤU TRÚC BỘ DỮ LIỆU\")\n",
        "print(\"=\"*30)\n",
        "sample_structure = data['train'][0]\n",
        "print(f\"Keys trong 1 mẫu: {list(sample_structure.keys())}\")\n",
        "# Mong đợi: ['words', 'tags']\n",
        "\n",
        "# 3. Kiểm tra số lượng mẫu\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"SỐ LƯỢNG MẪU (Câu)\")\n",
        "print(\"=\"*30)\n",
        "print(f\"Train set: {len(data['train'])} câu\")\n",
        "print(f\"Dev set:   {len(data['dev'])} câu\")\n",
        "print(f\"Test set:  {len(data['test'])} câu\")\n",
        "\n",
        "# 4. In 20 dòng đầu (Token - Tag) của mẫu đầu tiên\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"XEM CHI TIẾT CẤU TRÚC BIO (Câu đầu tiên tập Train)\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "first_sample = data['train'][0]\n",
        "df_sample = pd.DataFrame({\n",
        "    'Word': first_sample['words'],\n",
        "    'Tag': first_sample['tags']\n",
        "})\n",
        "\n",
        "print(df_sample.head(20))\n",
        "\n",
        "# 5. Xem nhanh vài câu khác\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"XEM NHANH 5 CÂU KHÁC\")\n",
        "print(\"=\"*30)\n",
        "for i in range(1, 6):\n",
        "    sample = data['train'][i]\n",
        "    # Ghép từ và tag lại xem cho gọn\n",
        "    # Chỉ in 100 ký tự đầu\n",
        "    text_preview = \" \".join([f\"{w}/{t}\" for w, t in zip(sample['words'], sample['tags'])])\n",
        "    print(f\"Câu {i}: {text_preview[:100]}...\")"
      ],
      "metadata": {
        "id": "2umSUPdDSlnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Thống kê số câu trong từng tập\n",
        "\n",
        "* Số câu trong train/dev/test\n",
        "\n",
        "* Số lượng token mỗi tập"
      ],
      "metadata": {
        "id": "QzAGo0jVSwjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Giả sử biến 'data' đã được tải thành công từ bước trước\n",
        "# data = {'train': [...], 'dev': [...], 'test': [...]}\n",
        "\n",
        "print(\">>> Đang tính toán thống kê...\")\n",
        "\n",
        "stats = []\n",
        "\n",
        "for split in ['train', 'dev', 'test']:\n",
        "    # 1. Lấy danh sách các câu trong tập hiện tại\n",
        "    samples = data[split]\n",
        "\n",
        "    # 2. Đếm số câu\n",
        "    num_sentences = len(samples)\n",
        "\n",
        "    # 3. Đếm tổng số token\n",
        "    # Duyệt qua từng câu, lấy độ dài của list 'words' và cộng dồn\n",
        "    num_tokens = sum(len(s['words']) for s in samples)\n",
        "\n",
        "    # 4. Tính độ dài trung bình (để biết câu dài hay ngắn)\n",
        "    avg_len = num_tokens / num_sentences if num_sentences > 0 else 0\n",
        "\n",
        "    # Lưu vào danh sách kết quả\n",
        "    stats.append({\n",
        "        'Tập dữ liệu': split.upper(),\n",
        "        'Số câu': num_sentences,\n",
        "        'Tổng số token': num_tokens,\n",
        "        'Trung bình (Token/Câu)': round(avg_len, 2)\n",
        "    })\n",
        "\n",
        "# Chuyển thành DataFrame hiển thị cho đẹp\n",
        "df_stats = pd.DataFrame(stats)\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"BẢNG THỐNG KÊ CHI TIẾT\")\n",
        "print(\"=\"*30)\n",
        "print(df_stats)\n",
        "\n",
        "# Vẽ biểu đồ so sánh số lượng câu (Optional)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(df_stats['Tập dữ liệu'], df_stats['Số câu'], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "plt.title('Số lượng câu trong các tập dữ liệu (PhoNER_COVID19)')\n",
        "plt.ylabel('Số câu')\n",
        "plt.bar_label(bars, fmt='%d')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pkrfpFkyS1QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Phân bố nhãn thực thể\n",
        "\n",
        "* Thống kê số lần xuất hiện của từng nhãn (B-PER, I-PER, B-LOC, I-LOC, B-ORG, I-ORG, O,...).\n",
        "\n",
        "* Vẽ biểu đồ bar chart phân bố nhãn.\n",
        "\n",
        "Trả lời:\n",
        "\n",
        "* Nhãn nào xuất hiện nhiều nhất?\n",
        "\n",
        "* Nhãn nào xuất hiện hiếm?"
      ],
      "metadata": {
        "id": "r33mYoFgS1vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# --- PHẦN 3: PHÂN TÍCH PHÂN BỐ NHÃN (LABEL DISTRIBUTION) ---\n",
        "\n",
        "# Giả sử biến 'data' đã có từ code trước (data['train'], data['dev'], data['test'])\n",
        "print(\">>> Đang phân tích nhãn trong tập TRAIN...\")\n",
        "\n",
        "# 1. Gom tất cả các nhãn trong tập Train lại thành 1 list duy nhất\n",
        "all_tags = []\n",
        "for sample in data['train']:\n",
        "    all_tags.extend(sample['tags'])\n",
        "\n",
        "# 2. Đếm số lượng mỗi nhãn\n",
        "tag_counts = Counter(all_tags)\n",
        "\n",
        "# Chuyển sang DataFrame để dễ quan sát và sắp xếp\n",
        "df_tags = pd.DataFrame.from_dict(tag_counts, orient='index', columns=['Count']).reset_index()\n",
        "df_tags.columns = ['Label', 'Count']\n",
        "\n",
        "# Tính tỷ lệ phần trăm\n",
        "total_tags = len(all_tags)\n",
        "df_tags['Percentage'] = (df_tags['Count'] / total_tags) * 100\n",
        "\n",
        "# Sắp xếp giảm dần\n",
        "df_tags = df_tags.sort_values(by='Count', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"BẢNG THỐNG KÊ TẦN SUẤT NHÃN\")\n",
        "print(\"=\"*30)\n",
        "print(df_tags)\n",
        "\n",
        "# 3. Vẽ biểu đồ (Bar Chart)\n",
        "# Lưu ý: Nhãn 'O' thường chiếm áp đảo (90%), nên ta sẽ vẽ 2 biểu đồ để nhìn rõ hơn.\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
        "\n",
        "# Biểu đồ 1: Toàn bộ nhãn (Bao gồm 'O')\n",
        "sns.barplot(data=df_tags, x='Label', y='Count', ax=ax[0], palette='viridis')\n",
        "ax[0].set_title('Phân bố toàn bộ nhãn (Bao gồm O)')\n",
        "ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=90)\n",
        "ax[0].set_ylabel('Số lượng')\n",
        "\n",
        "# Biểu đồ 2: Chỉ các nhãn thực thể (Loại bỏ 'O')\n",
        "df_entities = df_tags[df_tags['Label'] != 'O']\n",
        "sns.barplot(data=df_entities, x='Label', y='Count', ax=ax[1], palette='magma')\n",
        "ax[1].set_title('Phân bố các nhãn Thực thể (Bỏ qua O)')\n",
        "ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\n",
        "ax[1].set_ylabel('Số lượng')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Trả lời câu hỏi\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"KẾT LUẬN\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Nhãn nhiều nhất\n",
        "top_1 = df_tags.iloc[0]\n",
        "# Nhãn thực thể nhiều nhất (không tính O)\n",
        "top_entity = df_entities.iloc[0]\n",
        "# Nhãn hiếm nhất\n",
        "min_1 = df_tags.iloc[-1]\n",
        "\n",
        "print(f\"1. Nhãn xuất hiện nhiều nhất toàn cục: '{top_1['Label']}' ({top_1['Count']} lần - chiếm {top_1['Percentage']:.2f}%)\")\n",
        "print(f\"   -> Đây là nhãn 'Outside' (không phải thực thể).\")\n",
        "print(f\"\\n2. Nhãn THỰC THỂ xuất hiện nhiều nhất: '{top_entity['Label']}' ({top_entity['Count']} lần)\")\n",
        "print(f\"   -> Điều này phản ánh dataset tập trung vào thông tin này (thường là LOCATION hoặc PATIENT_ID).\")\n",
        "print(f\"\\n3. Nhãn xuất hiện HIẾM nhất: '{min_1['Label']}' ({min_1['Count']} lần)\")\n",
        "print(f\"   -> Cần lưu ý khi train, model có thể học kém ở nhãn này (Class Imbalance).\")"
      ],
      "metadata": {
        "id": "t82jHyA3S-dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Phân tích độ dài câu\n",
        "\n",
        "* Tính số token cho từng câu.\n",
        "\n",
        "* Tìm min / max / mean.\n",
        "\n",
        "Trả lời câu hỏi: padding tối ưu? có câu nào quá dài gây bất lợi cho LSTM?"
      ],
      "metadata": {
        "id": "ZeRWMh-2S-83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- PHẦN 4: PHÂN TÍCH ĐỘ DÀI CÂU (SEQUENCE LENGTH) ---\n",
        "\n",
        "print(\">>> Đang phân tích độ dài câu trong tập TRAIN...\")\n",
        "\n",
        "# 1. Tính số token cho từng câu\n",
        "# List chứa độ dài của từng câu\n",
        "sent_lengths = [len(sample['words']) for sample in data['train']]\n",
        "\n",
        "# Chuyển sang DataFrame để thống kê nhanh\n",
        "df_len = pd.DataFrame(sent_lengths, columns=['length'])\n",
        "\n",
        "# 2. Tìm Min / Max / Mean / Median\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"THỐNG KÊ ĐỘ DÀI CÂU\")\n",
        "print(\"=\"*30)\n",
        "stats = df_len.describe()\n",
        "print(stats)\n",
        "\n",
        "min_len = df_len['length'].min()\n",
        "max_len = df_len['length'].max()\n",
        "mean_len = df_len['length'].mean()\n",
        "median_len = df_len['length'].median()\n",
        "\n",
        "print(f\"\\n- Ngắn nhất (Min): {min_len} tokens\")\n",
        "print(f\"- Dài nhất (Max):  {max_len} tokens\")\n",
        "print(f\"- Trung bình (Mean): {mean_len:.2f} tokens\")\n",
        "print(f\"- Trung vị (Median): {median_len} tokens\")\n",
        "\n",
        "# 3. Tính các mốc Percentile (Quan trọng cho việc chọn Padding)\n",
        "p90 = np.percentile(sent_lengths, 90)\n",
        "p95 = np.percentile(sent_lengths, 95)\n",
        "p99 = np.percentile(sent_lengths, 99)\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"CÁC MỐC PHẦN TRĂM (PERCENTILES)\")\n",
        "print(\"=\"*30)\n",
        "print(f\"- 90% số câu có độ dài <= {p90} tokens\")\n",
        "print(f\"- 95% số câu có độ dài <= {p95} tokens\")\n",
        "print(f\"- 99% số câu có độ dài <= {p99} tokens\")\n",
        "\n",
        "# 4. Vẽ biểu đồ Histogram và Boxplot\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Histogram: Xem phân phối chuẩn hay lệch\n",
        "sns.histplot(sent_lengths, bins=30, kde=True, ax=ax[0], color='skyblue')\n",
        "ax[0].set_title('Phân phối độ dài câu (Histogram)')\n",
        "ax[0].set_xlabel('Số lượng token')\n",
        "ax[0].set_ylabel('Số lượng câu')\n",
        "ax[0].axvline(p95, color='r', linestyle='--', label=f'95% ({int(p95)})')\n",
        "ax[0].legend()\n",
        "\n",
        "# Boxplot: Phát hiện câu quá dài (Outliers)\n",
        "sns.boxplot(x=sent_lengths, ax=ax[1], color='lightgreen')\n",
        "ax[1].set_title('Biểu đồ hộp (Boxplot) phát hiện Outlier')\n",
        "ax[1].set_xlabel('Số lượng token')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- TRẢ LỜI CÂU HỎI ---\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"TRẢ LỜI CÂU HỎI\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Câu 1: Padding tối ưu?\n",
        "# Thường chọn mốc bao phủ 95% đến 99% dữ liệu, và làm tròn lên lũy thừa của 2 (32, 64, 128...)\n",
        "optimal_padding = int(p99)\n",
        "if optimal_padding < 64: suggested_maxlen = 64\n",
        "elif optimal_padding < 128: suggested_maxlen = 128\n",
        "elif optimal_padding < 256: suggested_maxlen = 256\n",
        "else: suggested_maxlen = 512\n",
        "\n",
        "print(f\"1. Padding tối ưu (max_len):\")\n",
        "print(f\"   - Nếu chọn theo 99% dữ liệu: {int(p99)} tokens.\")\n",
        "print(f\"   - Khuyến nghị cấu hình Model: max_len = {suggested_maxlen}.\")\n",
        "print(f\"   (Lý do: Bao phủ gần hết dữ liệu mà không tốn tài nguyên cho các câu outlier quá dài).\")\n",
        "\n",
        "# Câu 2: Có câu nào quá dài gây bất lợi cho LSTM?\n",
        "outlier_threshold = p99 + 20 # Giả sử dài hơn mốc 99% một đoạn là bất lợi\n",
        "long_sentences = df_len[df_len['length'] > 100].count().values[0] # LSTM thường bắt đầu khó khăn > 100 bước\n",
        "print(f\"\\n2. Có câu nào quá dài gây bất lợi cho LSTM không?\")\n",
        "if max_len > 100:\n",
        "    print(f\"   -> CÓ. Câu dài nhất là {max_len} tokens.\")\n",
        "    print(f\"   -> Số lượng câu > 100 tokens: {long_sentences} câu.\")\n",
        "    print(\"   -> Tác động: Với LSTM thuần (Vanilla LSTM), chuỗi > 100 tokens dễ gây ra 'Vanishing Gradient' (mất mát thông tin đầu câu).\")\n",
        "    print(\"   -> Giải pháp: Dùng Bi-LSTM (2 chiều) + Attention Mechanism hoặc cắt ngắn (Truncate) về mốc 99%.\")\n",
        "else:\n",
        "    print(\"   -> KHÔNG. Dữ liệu này có độ dài vừa phải, LSTM xử lý tốt.\")"
      ],
      "metadata": {
        "id": "KxunIWjyTLmM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}